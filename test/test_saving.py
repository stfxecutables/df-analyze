from __future__ import annotations

# fmt: off
import sys  # isort: skip
from pathlib import Path  # isort: skip
ROOT = Path(__file__).resolve().parent.parent  # isort: skip
sys.path.append(str(ROOT))  # isort: skip
# fmt: on


import json
import sys
from copy import deepcopy
from dataclasses import is_dataclass
from pathlib import Path
from tempfile import TemporaryDirectory
from typing import (
    Any,
    Tuple,
)

import numpy as np
from numpy import ndarray
from pandas import DataFrame, Series

from df_analyze.cli.cli import ProgramOptions
from df_analyze.hypertune import EvaluationResults
from df_analyze.preprocessing.inspection.inspection import InspectionResults
from df_analyze.selection.embedded import EmbedSelected
from df_analyze.selection.wrapper import WrapperSelected
from df_analyze.testing.datasets import TestDataset, fast_ds


@fast_ds
def test_random_options(dataset: Tuple[str, TestDataset]) -> None:
    dsname, ds = dataset
    for _ in range(100):
        ProgramOptions.random(ds)


@fast_ds
def test_hashing(dataset: Tuple[str, TestDataset]) -> None:
    dsname, ds = dataset
    for _ in range(100):
        with TemporaryDirectory() as tempdir:
            outdir = Path(tempdir)
            opts1 = ProgramOptions.random(ds, outdir=outdir)
            opts2 = ProgramOptions.random(ds, outdir=outdir)
            opts11 = deepcopy(opts1)
            opts22 = deepcopy(opts2)
            hsh1 = opts1.hash()
            hsh2 = opts2.hash()
            hsh11 = opts11.hash()
            hsh22 = opts22.hash()
            assert hsh1 != hsh2
            assert hsh1 == hsh11
            assert hsh2 == hsh22


def are_equal(obj1: Any, obj2: Any) -> bool:
    if is_dataclass(obj1) and is_dataclass(obj2):
        if obj1.__dict__.keys() != obj2.__dict__.keys():
            return False

        for key in obj1.__dict__.keys():
            # not sure why, but this appears when you add a mixin to the Enum
            if key == "__objclass__":
                continue
            attr1 = obj1.__dict__[key]
            attr2 = obj2.__dict__[key]
            try:
                eq = are_equal(attr1, attr2)
                if not eq:
                    return False
            except RecursionError:
                raise ValueError(
                    f"Failed to compare objects on key={key} due to infinite recursion. Compared attribute types:\n"
                    f"attr1={type(attr1)}, attr2={type(attr2)}"
                )
        return True
    elif isinstance(obj1, ndarray) and isinstance(obj2, ndarray):
        return (obj1.ravel().round(8) == obj2.ravel().round(8)).all()
    elif isinstance(obj1, (DataFrame, Series)) and isinstance(obj2, (DataFrame, Series)):
        return np.all((obj1.round(8).values == obj2.round(8).values)).item()
    else:
        return obj1 == obj2


@fast_ds
def test_json(dataset: Tuple[str, TestDataset]) -> None:
    dsname, ds = dataset
    try:
        with TemporaryDirectory() as tempdir:
            for _ in range(10):
                outdir = Path(tempdir)
                opts = ProgramOptions.random(ds, outdir=outdir)
                opts.to_json()
                assert outdir.exists()
                assert opts.program_dirs.options is not None
                assert opts.program_dirs.options.exists()
                opts2 = ProgramOptions.from_json(opts.program_dirs.root)
                for attr in opts.__dict__:
                    attr1 = getattr(opts, attr)
                    attr2 = getattr(opts2, attr)
                    if not are_equal(attr1, attr2):
                        raise ValueError(
                            f"Saved attribute {attr}: {attr2} does not equal initial "
                            f"value of: {attr1}"
                        )
    except Exception as e:
        raise e


@fast_ds
def test_inspection_json(dataset: Tuple[str, TestDataset]) -> None:
    dsname, ds = dataset
    with TemporaryDirectory() as tempdir:
        inspections = ds.inspect(load_cached=True, force=False)
        outdir = Path(tempdir)
        outfile = outdir / "inspections.json"
        inspections.to_json(outfile)
        loaded = InspectionResults.from_json(outfile)
        assert isinstance(loaded, InspectionResults)
        for attr in inspections.__dict__:
            attr1 = getattr(inspections, attr)
            attr2 = getattr(loaded, attr)
            if not are_equal(attr1, attr2):
                raise ValueError(
                    f"Saved attribute {attr}: {attr2} does not equal initial value of: {attr1}"
                )


@fast_ds
def test_embed_json(dataset: Tuple[str, TestDataset]) -> None:
    dsname, ds = dataset
    tempdir = TemporaryDirectory()
    selected = EmbedSelected.random(ds)
    outdir = Path(tempdir.name)
    outfile = outdir / "embed.json"
    outfile.write_text(selected.to_json(), encoding="utf-8")
    loaded = EmbedSelected.from_json(outfile)
    assert isinstance(loaded, EmbedSelected)
    for attr in selected.__dict__:
        attr1 = getattr(selected, attr)
        attr2 = getattr(loaded, attr)
        if not are_equal(attr1, attr2):
            raise ValueError(
                f"Saved attribute {attr}: {attr2} does not equal initial value of: {attr1}"
            )


@fast_ds
def test_wrap_json(dataset: Tuple[str, TestDataset]) -> None:
    dsname, ds = dataset
    tempdir = TemporaryDirectory()
    selected = WrapperSelected.random(ds)
    outdir = Path(tempdir.name)
    outfile = outdir / "wrap.json"
    outfile.write_text(selected.to_json(), encoding="utf-8")
    loaded = WrapperSelected.from_json(outfile)
    assert isinstance(loaded, WrapperSelected)
    for attr in selected.__dict__:
        attr1 = getattr(selected, attr)
        attr2 = getattr(loaded, attr)
        if not are_equal(attr1, attr2):
            raise ValueError(
                f"Saved attribute {attr}: {attr2} does not equal initial value of: {attr1}"
            )


@fast_ds
def test_eval_save(dataset: Tuple[str, TestDataset]) -> None:
    dsname, ds = dataset
    if dsname in ["dgf_96f4164d-956d-4c1c-b161-68724eb0ccdc", "internet_usage"]:
        return  # defective target
    tempdir = TemporaryDirectory()
    outdir = Path(tempdir.name)
    try:
        for _ in range(10):
            options = ProgramOptions.random(ds, outdir=outdir)
            selected = EvaluationResults.random(ds, options)

            selected.save(root=outdir, fold_idx=None)
            loaded = EvaluationResults.load(root=outdir)

            assert isinstance(loaded, EvaluationResults)
            for attr in selected.__dict__:
                if attr == "results":  # broken jsonpickle again...
                    continue
                attr1 = getattr(selected, attr)
                attr2 = getattr(loaded, attr)
                if not are_equal(attr1, attr2):
                    raise ValueError(
                        f"Saved attribute {attr}: {attr2} does not equal initial value of: {attr1}"
                    )
    except Exception as e:
        raise ValueError(f"Got error saving eval results for data {dsname}:\n{e}")
    finally:
        tempdir.cleanup()


@fast_ds
def test_eval_preds_save(dataset: Tuple[str, TestDataset]) -> None:
    dsname, ds = dataset
    if dsname in ["dgf_96f4164d-956d-4c1c-b161-68724eb0ccdc", "internet_usage"]:
        return  # defective target
    tempdir = TemporaryDirectory()
    outdir = Path(tempdir.name)
    try:
        for _ in range(10):
            options = ProgramOptions.random(ds, outdir=outdir)
            selected = EvaluationResults.random(ds, options)
            preds = [result.to_preds() for result in selected.results]

            selected.save(root=outdir, fold_idx=None)
            loaded_preds = EvaluationResults.load_preds(root=outdir)

            for i, pred in enumerate(preds):
                for key in pred.__dict__.keys():
                    original = pred.__dict__[key]
                    loaded = loaded_preds[i].__dict__[key]
                    if key == "params":
                        if str(original) == str(loaded):
                            continue
                    if not are_equal(original, loaded):
                        raise ValueError(
                            f"Saved values at key: '{key}' are not equal after loading:\n"
                            "Before:\n"
                            f"{original}\n"
                            "Loaded:\n"
                            f"{loaded}"
                        )

                probs_test = pred.probs_test
                probs_test_loaded = loaded_preds[i].__dict__["probs_test"]
                probs_train = pred.probs_train
                probs_train_loaded = loaded_preds[i].__dict__["probs_train"]

                preds_test = pred.preds_test
                preds_test_loaded = loaded_preds[i].__dict__["preds_test"]
                preds_train = pred.preds_train
                preds_train_loaded = loaded_preds[i].__dict__["preds_train"]

                assert preds_test is not None, "Null / missing preds"
                assert preds_train is not None, "Null / missing preds"
                assert preds_test_loaded is not None, "Null / missing preds"
                assert preds_train_loaded is not None, "Null / missing preds"

                if ds.is_classification:
                    assert probs_test is not None, "Null / missing probs"
                    assert probs_train is not None, "Null / missing probs"
                    assert probs_test_loaded is not None, "Null / missing probs"
                    assert probs_train_loaded is not None, "Null / missing probs"

                    assert len(probs_test) == len(preds_test), (
                        "Test probs and preds mismatch"
                    )
                    assert len(probs_test_loaded) == len(preds_test_loaded), (
                        "Loaded test probs and preds mismatch"
                    )

            # Check predictions load as plain json for use outside of df-analyze
            json_str = (outdir / "prediction_results.json").read_text()
            try:
                json.loads(json_str)
            except Exception as e:
                raise ValueError("Could not load predictions as plain json.") from e

    except Exception as e:
        raise ValueError(
            f"Got error saving prediction results for data {dsname}:\n{e}"
        ) from e
    finally:
        tempdir.cleanup()


@fast_ds
def test_descs_save(dataset: Tuple[str, TestDataset]) -> None:
    dsname, ds = dataset
    if dsname in ["dgf_96f4164d-956d-4c1c-b161-68724eb0ccdc", "internet_usage"]:
        return  # defective target
    tempdir = TemporaryDirectory()
    outdir = Path(tempdir.name)

    try:
        options = ProgramOptions.random(ds, outdir=outdir)
        preds = ds.prepared(load_cached=True)
        df_cont, df_cat, df_targ = preds.describe_features()
        options.program_dirs.save_feature_descriptions(df_cont, df_cat, df_targ)

    except Exception as e:
        raise ValueError(
            f"Got error saving prediction results for data {dsname}:\n{e}"
        ) from e
    finally:
        tempdir.cleanup()
